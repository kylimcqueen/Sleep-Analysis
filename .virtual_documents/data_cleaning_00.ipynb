


#!pip install pytest


import pandas as pd
import os #for viewing HTML in web browser
import matplotlib.pyplot as plt
from typing import Tuple, List # If you have functions that return more than one value, 
# they will be returned in a tuple and you need this  to write that out in typehints


from IPython.display import display, HTML
# For pandas DataFrames
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)  # Show all columns


#Open df in the browser - easiest way to view all the data in this large dataset
def to_html(df: pd.DataFrame) -> str:
    '''
    Render the df in HTML. Return the full HTML address. You can paste the HTML address in the 
    address bar to see the df at any time.
    '''
    df.to_html('df_view.html')
    full_path = os.path.abspath('df_view.html')
    print(full_path)


#Load file
# work_filepath = "C:\Users\kylimcqueen\Downloads\all_nts_animals_baseline_tall_00.csv"
#mac_filepath = '/Users/kyli/Desktop/Neurotrauma/all_nts_animals_baseline_tall_00.csv'
filepath=("\\Users\\kylimcqueen\\Downloads\\all_nts_animals_postinjury_tall_00.csv")


#Check that we grabbed the filepath
print(filepath)


#Create df
df_original = pd.read_csv(filepath)


#Look at the df
to_html(df_original)








#Check for excluded animals
def whats_in_the_col(df: pd.DataFrame, data_name: str, col_name: str) -> Tuple[int, List[str]]:
    '''
    Counts the number of cells in the specified column of the dataframe that don't 
    exactly match the string "Included".
    '''
    assert len(df[col_name]) == len(df) #Assert we're accessing the whole column

    #Assert all values in the column are of the same type (string)
    #.map(type) gets the type of each value
    #.nunique() counts number of unique types
    assert df[col_name].map(type).nunique() == 1, "Not all values are the same type"
                            
    # Get values that don't match the data_name
    non_matching_mask: list = df[col_name] != data_name
    
    # Count how many cells don't match
    count: int = non_matching_mask.sum()
    
    # Get the actual values that don't match
    non_matching_values: list = df.loc[non_matching_mask, col_name].unique().tolist()
    
    return count, non_matching_values


count, no_match_list = whats_in_the_col(df_original, "Included", "Included")


print(count) # If count = 0 then every item in the column matches the data_name, "Included"
print(no_match_list) # If no_match_list is empty, then every item in the column matches the data name, "Included"





def trim_nan_edges(df: pd.DataFrame, cohort_col: pd.Series, mouse_col: pd.Series, data_col:pd.Series):
    '''
    Get rid of the leading and lagging NaNs at the beginning and end of the set of rows that represent 
    all observations for each mouse. Do not get rid of NaNs in the middle of observations. 
    All mice within the same cohort have the same number of leading and lagging NaNs.
    '''
    trimmed_groups: list = []

    # Nested for loop to loop over each UniqueMouse within each Cohort
    df_grouped = df.groupby(cohort_col) # Group by Cohorts
    for cohort_name, cohort_group in df_grouped: # Loop over each cohort    
        for mouse_name, mouse_group in cohort_group.groupby(mouse_col): # Loop over UniqueMouse
    
            mouse_group = mouse_group.reset_index(drop=True) # Reset the index for the rows of a UniqueMouse
            # Create a new list called valid_mask where each entry is a boolean value where T = 1 = a number and
            # F = 0 = a NaN for that Uniquemouse's SleepPercent data
            valid_mask = mouse_group[data_col].notna() 

            #If there are any NaNs
            if valid_mask.any():
                # first_valid_idx is the first index that returns true - the first number after leading NaNs
                first_valid_idx = valid_mask.idxmax()
                # last_valid_idx is the first index going backwards that returns true - the last number before lagging NaNs
                last_valid_idx = valid_mask[valid_mask].index[-1]
    

                # Take the first_valid_idx and last_valid_idx, match them to corresponding actual values in the UniqueMouse
                # SleepPercent data, and put the actual SleepPercent values in a new list called trimmed
                trimmed = mouse_group.iloc[first_valid_idx:last_valid_idx + 1].copy() 
                # Add the trimmed SleepPercent column from one UniqueMouse to the trimmed_groups list, which is a list
                # of trimmed values for all UniqueMice across all Cohorts
                trimmed_groups.append(trimmed) # Add the trimmed SleepPercent column to the 
    
    trimmed_df = pd.concat(trimmed_groups, ignore_index=True)
    return trimmed_df


# Run trim_nan_edges function
trimmed_df = trim_nan_edges(df_original, "Cohort", "UniqueMouse", "PercentSleep")








# Get the columns
def relevant_cols(df: pd.DataFrame):
    ''' Function to iterate through the columns of a given dataframe and choose which columns to add to a list. 
    List will be used in upcoming calculations. Useful for masking or performing operations on a set of columns.'''
    relevant_cols = []
    for col in df.columns:
        print(f'\nColumn name: {col} ')
        relevance = input(f'\nIs this column relevant for the next calculation? Type y or n ')
        if relevance == 'y':
            relevant_cols.append(col)
    return relevant_cols



col_list = relevant_cols(trimmed_df)


# Check that the list of columns you generated looks right
print(col_list)


# Remove unnecessary columns
remove_cols_df = trimmed_df.loc[:,col_list]
remove_cols_df





duplicate_count = remove_cols_df.duplicated().sum()
print(f' Number of duplicate rows: {duplicate_count}')








print(f'DataFrame information: \n{remove_cols_df.info()}')


'''Convert columns with type 'object' to categories because all data in these columns are limited unique values that repeat (like group identifiers).
Category designation stores data as categorical/enumerated values, is more memory-efficient, and is faster for operations on columns.'''
def turn_obj_cols_to_categories(df: pd.DataFrame) -> pd.DataFrame:
    obj_cols = df.select_dtypes(include='object').columns
    for col in obj_cols:
        df[col] = df[col].astype('category')
    return df


turn_obj_cols_to_categories(remove_cols_df)


remove_cols_df.dtypes





## Understand NaN patterns


### How prevalent are NaNs


# See the % of NaNs per total data for each UniqueMouse
na_summary = remove_cols_df["PercentSleep"].isna().groupby(remove_cols_df["UniqueMouse"]).mean()
# Print the % NaNs per total data for each UniqueMouse from largest to smallest
print(f' There are a total of {len(na_summary)} mice.')
print(type(na_summary.sort_values(ascending=False)))
print(na_summary.sort_values(ascending=False))

'''
For postinjury, there are 71 total mice. Only 14 mice have an average of more than 0% NaN values.  
However, the vast majority of animals with NaN values are from Cohort 5. '''





# Interpolation
remove_cols_df["PercentSleep"] = remove_cols_df.groupby("UniqueMouse", observed=False)["PercentSleep"].transform(lambda x: x.interpolate())


# Check that there's no NaNs now
na_summary = remove_cols_df["PercentSleep"].isna().groupby(remove_cols_df["UniqueMouse"]).mean()
print(f' There are a total of {len(na_summary)} mice.')
print(type(na_summary.sort_values(ascending=False)))
print(na_summary.sort_values(ascending=False))



# Make a dictionary of dataframes so that you can view each cohort independently
# Each key is the name of the cohort (ex: C2, C3, etc) and the value is the datafarme of that cohort only
cohorts_dict = {cohort: data for cohort, data in remove_cols_df.groupby('Cohort')}


# Check a few cohorts to make sure they look normal
display(cohorts_dict['C5'])


# Check if there are remaining NaNs
